# ğŸ  ReducciÃ³n de Dimensionalidad y Clustering en California Housing Dataset

## ğŸ¯ Objetivo
Explorar la estructura de los datos del **California Housing Dataset** mediante **reducciÃ³n de dimensionalidad** y **clustering no supervisado**, evaluando si es posible identificar patrones similares a la variable categÃ³rica `ocean_proximity` sin usarla directamente en el entrenamiento.

---

## ğŸ“‚ Dataset
- **Fuente**: California Housing Dataset.
- **Filas**: 20.640
- **Columnas**: 9 (numÃ©ricas + `ocean_proximity` categÃ³rica)
- **Variable categÃ³rica objetivo (solo para evaluaciÃ³n)**: `ocean_proximity`.

---

## ğŸ›  MetodologÃ­a

### 1. Preprocesamiento
- CodificaciÃ³n de la variable categÃ³rica (`OneHotEncoder`).
- Escalado de variables numÃ©ricas (`StandardScaler`).
- Pipeline unificado con `ColumnTransformer`.

---

### 2. PCA (AnÃ¡lisis de Componentes Principales)
- ReducciÃ³n a **2 componentes**.
- **Varianza explicada acumulada**: ~62%.
- VisualizaciÃ³n coloreando por `ocean_proximity` â†’ alto solapamiento.
- Clustering con **K-Means (K=5)** â†’ **ARI = 0.125** â†’ baja similitud con categorÃ­as reales.

---

### 3. t-SNE (ReducciÃ³n de Dimensionalidad No Lineal)
- ReducciÃ³n a 2 dimensiones preservando relaciones no lineales.
- ParametrizaciÃ³n inicial (`perplexity=30`, `learning_rate=200`) mostrÃ³ mayor separaciÃ³n visual.
- OptimizaciÃ³n de hiperparÃ¡metros:
  - Mejor combinaciÃ³n: **perplexity=30, learning_rate=500**.
  - **ARI con K-Means sobre t-SNE optimizado = 0.419** â†’ mejora significativa frente a PCA.

---

### 4. Clustering (K-Means)
- **NÃºmero de clusters (K)**: igual al nÂº de categorÃ­as reales (5).
- EvaluaciÃ³n con **Adjusted Rand Index (ARI)** usando `ocean_proximity` como referencia.
- Tabla de contingencia â†’ varios clusters representan claramente categorÃ­as especÃ­ficas (ej. `INLAND`, `NEAR BAY`).

---

## ğŸ“Š Resultados

| TÃ©cnica                      | ARI   | Observaciones |
|------------------------------|-------|---------------|
| PCA + K-Means                | 0.125 | Mucho solapamiento, estructura lineal insuficiente. |
| t-SNE inicial + K-Means      | 0.341 | Mejora notable, grupos mÃ¡s definidos visualmente. |
| t-SNE optimizado + K-Means   | 0.419 | Mejor separaciÃ³n, varios clusters alineados con categorÃ­as reales. |

---

## ğŸ“ˆ VisualizaciÃ³n Comparativa
- **Izquierda**: t-SNE optimizado coloreado por categorÃ­as reales.  
- **Derecha**: t-SNE optimizado coloreado por clusters K-Means.  
*(Inserta aquÃ­ las imÃ¡genes generadas en el notebook)*

---

## ğŸ§  Conclusiones
- PCA es Ãºtil para visualizaciÃ³n y reducciÃ³n rÃ¡pida, pero limitado a patrones lineales.
- t-SNE capta relaciones no lineales, logrando mejor separaciÃ³n en este dataset.
- Aun sin usar `ocean_proximity` para entrenar, el clustering detectÃ³ parte de su estructura.
- ARI = 0.419 â†’ correlaciÃ³n moderada entre clusters y categorÃ­as reales.

---

## ğŸš€ PrÃ³ximos pasos
- Probar **UMAP** como alternativa mÃ¡s rÃ¡pida y escalable a t-SNE.
- Enriquecer las features con transformaciones geogrÃ¡ficas para mejorar aÃºn mÃ¡s el clustering.
- Usar mÃ©todos de clustering mÃ¡s flexibles (Gaussian Mixtures, DBSCAN optimizado).

---

## âš™ï¸ Requisitos de ejecuciÃ³n
Instalar dependencias necesarias:
```bash
pip install -r requirements.txt
